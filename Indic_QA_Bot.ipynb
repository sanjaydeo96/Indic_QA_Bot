{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install all necessary dependencies"
      ],
      "metadata": {
        "id": "3efpr90X67VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api faiss-cpu numpy tqdm sentence-transformers langdetect transformers rank-bm25 indic-nlp-library sacrebleu deep-translator datasets"
      ],
      "metadata": {
        "id": "2ri4r_ww67oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import all necessary dependencies"
      ],
      "metadata": {
        "id": "vFZ-gCKK6kc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "import re\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langdetect import detect\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from deep_translator import GoogleTranslator\n",
        "import torch"
      ],
      "metadata": {
        "id": "BeGUkrPo29_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration Parameters\n",
        "\n",
        "This section outlines the configurable parameters used in the RAG pipeline. To adjust the system's behavior, modify the following variables as needed:\n",
        "* MODEL_NAME\n",
        "* LANGUAGE\n",
        "* TOP_K\n",
        "\n",
        "Don't forget to add key:value pair if you add extra varibles in **Topic Mapping and Languange Code Mapping**"
      ],
      "metadata": {
        "id": "a69EBfIc3PKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Selection (Uncomment the desired model)\n",
        "# MODEL_NAME = \"t5-small\"\n",
        "MODEL_NAME = \"t5-base\"\n",
        "# MODEL_NAME = \"t5-large\"\n",
        "\n",
        "# Language Selection (Uncomment the desired language)\n",
        "#LANGUAGE = \"hi\"  # Hindi\n",
        "LANGUAGE = \"en\"  # English\n",
        "# LANGUAGE = \"bn\"  # Bengali\n",
        "# LANGUAGE = \"mr\" # Marathi\n",
        "# LANGUAGE = \"ta\" #Tamil\n",
        "#LANGUAGE = \"te\" #Telugu\n",
        "\n",
        "# Number of retrieved documents\n",
        "\n",
        "TOP_K = 5\n",
        "\n",
        "# Language Code Mapping\n",
        "LANGUAGE_MAPPING = {\n",
        "    \"hi\": \"hi\",  # Hindi\n",
        "    \"en\": \"en\",  # English\n",
        "    \"bn\": \"bn\",  # Bengali\n",
        "    \"mr\": \"mr\", # Marathi\n",
        "    \"ta\": \"ta\", #Tamil\n",
        "    \"te\": \"te\", #Telugu\n",
        "}\n",
        "\n",
        "# Topic Mapping\n",
        "TOPIC_MAPPING = {\n",
        "    \"hi\": \"संस्कृति\", # Culture in Hindi\n",
        "    \"en\": \"Culture\",\n",
        "    \"bn\": \"সংস্কৃতি\",\n",
        "    \"mr\": \"संस्कृती\",\n",
        "    \"ta\": \"பண்பாடு\",\n",
        "    \"te\": \"సంస్కృతి\",\n",
        "}"
      ],
      "metadata": {
        "id": "Wc8bKYir3HU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The RAG pipeline encompasses these key functions:**\n",
        "\n",
        "* Wikipedia Scraping\n",
        "* Data Preprocessing\n",
        "* FAISS-based Indexing (Dense Retrieval)\n",
        "* Language Detection and Translation\n",
        "* RAG Response Generation"
      ],
      "metadata": {
        "id": "dZEoKBDZ4ozS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Wikipedia Scraping\n",
        "def scrape_wikipedia(pages=5000, language=LANGUAGE):\n",
        "  wiki_lang = LANGUAGE_MAPPING.get(language, \"en\")  # Default to English if not found\n",
        "  topic = TOPIC_MAPPING.get(language, \"Culture\")\n",
        "  wiki = wikipediaapi.Wikipedia(language=wiki_lang, user_agent=\"RAG-INDI/1.0 (sanjaydeo96@gmail.com)\")\n",
        "  page = wiki.page(topic)\n",
        "\n",
        "  if not page.exists():\n",
        "      print(f\"⚠️ Error: Wikipedia page '{topic}' does not exist in {wiki_lang}.\")\n",
        "      return []\n",
        "\n",
        "  articles = []\n",
        "  for link in page.links.values():\n",
        "      if len(articles) >= pages:\n",
        "          break\n",
        "      try:\n",
        "          sub_page = wiki.page(link.title)\n",
        "          if sub_page.exists():\n",
        "              text = sub_page.text\n",
        "              text = preprocess_text(text)\n",
        "              if len(text.split()) > 50:\n",
        "                  articles.append(text)\n",
        "      except Exception as e:\n",
        "          print(f\"Error retrieving page '{link.title}': {e}\")\n",
        "\n",
        "  return articles\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  text = re.sub(r'\\[[0-9]*\\]', '', text)\n",
        "  return text.strip()\n",
        "\n",
        "# Step 3: Indexing using FAISS (Dense Retrieval)\n",
        "class Indexing:\n",
        "  def __init__(self, documents, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\"):\n",
        "      self.documents = [preprocess_text(doc) for doc in documents]\n",
        "      self.embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "      self.doc_embeddings = np.array(self.embedding_model.encode(self.documents), dtype=np.float32)\n",
        "      self.index = faiss.IndexFlatL2(self.doc_embeddings.shape[1])\n",
        "      self.index.add(self.doc_embeddings)\n",
        "\n",
        "  def retrieve(self, query, k=TOP_K):\n",
        "      query_embedding = np.array(self.embedding_model.encode([query]), dtype=np.float32)\n",
        "      _, top_faiss_idx = self.index.search(query_embedding, k)\n",
        "      return [self.documents[i] for i in top_faiss_idx[0]]\n",
        "\n",
        "# Step 4: Language Detection & Translation\n",
        "def detect_and_translate(query, target_lang=\"en\"):\n",
        "  lang = detect(query)\n",
        "  if lang != target_lang:\n",
        "      translated_query = GoogleTranslator(source=lang, target=target_lang).translate(query)\n",
        "      return translated_query, lang\n",
        "  return query, lang\n",
        "\n",
        "# Step 5: RAG Response Generation (T5)\n",
        "def generate_response(query, retrieved_docs, language, model_name=MODEL_NAME, max_response_length=200):\n",
        "  try:\n",
        "      tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "      model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      model.to(device)\n",
        "\n",
        "      context = \" \".join(retrieved_docs)\n",
        "      max_context_length = 512\n",
        "      if len(context) > max_context_length:\n",
        "          context = context[:max_context_length]\n",
        "      prompt = f\"Answer the following question based on the context: Question: {query} Context: {context}\"\n",
        "\n",
        "      inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "      outputs = model.generate(\n",
        "          **inputs,\n",
        "          max_length=max_response_length,\n",
        "          num_beams=4,\n",
        "          early_stopping=True,\n",
        "          num_return_sequences=1\n",
        "      )\n",
        "\n",
        "      response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "      if language != \"en\":\n",
        "          response = GoogleTranslator(source=\"en\", target=language).translate(response)\n",
        "\n",
        "      del inputs\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      return response\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"Error generating response: {e}\")\n",
        "      return \"An error occurred while generating the response.\""
      ],
      "metadata": {
        "id": "h2ddS2fh3ilp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runinng pipeline by choosing sample **QUERY** variable from below cell"
      ],
      "metadata": {
        "id": "As_rTjFy31Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cultural Query 1: World Festivals\n",
        "QUERY = \"दुनिया में सबसे लोकप्रिय त्योहार कौन सा है?\" if LANGUAGE == \"hi\" else \"What is the most popular festival in the world?\" if LANGUAGE == \"en\" else \"পৃথিবীতে সবচেয়ে জনপ্রিয় উৎসব কোনটি?\" if LANGUAGE == \"bn\" else \"जगातील सर्वात लोकप्रिय सण कोणता आहे?\" if LANGUAGE == \"mr\" else \"உலகில் மிகவும் பிரபலமான பண்டிகை எது?\" if LANGUAGE == \"ta\" else \"ప్రపంచంలో అత్యంత ప్రజాదరణ పొందిన పండుగ ఏది?\" if LANGUAGE == \"te\" else \"What is the most popular festival in the world?\"\n",
        "# Cultural Query 2: World Music Genres\n",
        "#QUERY = \"दुनिया में सबसे लोकप्रिय संगीत शैली कौन सी है?\" if LANGUAGE == \"hi\" else \"What is the most popular music genre in the world?\" if LANGUAGE == \"en\" else \"পৃথিবীতে সবচেয়ে জনপ্রিয় সঙ্গীত ধারা কোনটি?\" if LANGUAGE == \"bn\" else \"जगातील सर्वात लोकप्रिय संगीत प्रकार कोणता आहे?\" if LANGUAGE == \"mr\" else \"உலகில் மிகவும் பிரபலமான இசை வகை எது?\" if LANGUAGE == \"ta\" else \"ప్రపంచంలో అత్యంత ప్రజాదరణ పొందిన సంగీత శైలి ఏది?\" if LANGUAGE == \"te\" else \"What is the most popular music genre in the world?\"\n",
        "# Cultural Query 3: World Dance Forms\n",
        "#QUERY = \"दुनिया में सबसे लोकप्रिय नृत्य रूप कौन सा है?\" if LANGUAGE == \"hi\" else \"What is the most popular dance form in the world?\" if LANGUAGE == \"en\" else \"পৃথিবীতে সবচেয়ে জনপ্রিয় নৃত্যশৈলী কোনটি?\" if LANGUAGE == \"bn\" else \"जगातील सर्वात लोकप्रिय नृत्य प्रकार कोणता आहे?\" if LANGUAGE == \"mr\" else \"உலகில் மிகவும் பிரபலமான நடன வடிவம் எது?\" if LANGUAGE == \"ta\" else \"ప్రపంచంలో అత్యంత ప్రజాదరణ పొందిన నృత్య రూపం ఏది?\" if LANGUAGE == \"te\" else \"What is the most popular dance form in the world?\""
      ],
      "metadata": {
        "id": "sumZ23dE3zGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Running the Pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Scraping Wikipedia...\")\n",
        "    documents = scrape_wikipedia()\n",
        "\n",
        "    if not documents:\n",
        "        print(\"No documents found. Exiting...\")\n",
        "        exit()\n",
        "\n",
        "    print(\"Indexing Documents...\")\n",
        "    indexer = Indexing(documents)\n",
        "\n",
        "    query = QUERY\n",
        "    translated_query, original_lang = detect_and_translate(query)\n",
        "\n",
        "    print(f\"Query Detected Language: {original_lang}\")\n",
        "    print(f\"Translated Query: {translated_query}\")\n",
        "\n",
        "    retrieved_docs = indexer.retrieve(translated_query, k=TOP_K)\n",
        "\n",
        "    print(\"Generating Response...\")\n",
        "    response = generate_response(translated_query, retrieved_docs, original_lang)\n",
        "\n",
        "    print(\"\\nFinal Response:\", response)"
      ],
      "metadata": {
        "id": "WWsjhpZC3ngD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}